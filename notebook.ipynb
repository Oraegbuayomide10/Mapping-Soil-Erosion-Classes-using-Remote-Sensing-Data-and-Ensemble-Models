{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ArtPr22GvxsO",
        "Dl6W0OG3wItX",
        "9JIVaS0WwudZ",
        "wFLsrEhCmSWw",
        "r5VC6fsYeYtB",
        "0je8I3dT3Ksy",
        "wxIvsCctUtoq",
        "kKqeDuQHxmAK",
        "n5mJa9maQri1",
        "pag7sBbhQxKF",
        "jUs9Be0TR4zO",
        "FT4aYVwGRY8o",
        "uEjknlYzSoDt",
        "9hjKEhuMTW2x",
        "c7iACj890rZk",
        "AbAFt5KuWsIm",
        "9DIZuy-7XJ3H",
        "_ruP9LYyXuYV",
        "5uuv1h-6bYj7",
        "yKJUc8ILb3GZ",
        "O3Q1KsM7cjGg",
        "koOHhLWTHHYK"
      ],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installation and Import of Libraries"
      ],
      "metadata": {
        "id": "ArtPr22GvxsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "X6iujRFVeVtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade linear-tree\n"
      ],
      "metadata": {
        "id": "3XhzmZ6XoNWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6kEXC_3huWa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
        "import random\n",
        "random.state= 42\n",
        "np.random.seed=42\n",
        "random.seed=42\n",
        "from sklearn.tree import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from shapely.geometry import Polygon, Point\n",
        "import geopandas as gpd\n",
        "import xgboost as xgb\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "from sklearn.feature_selection import RFE\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import warnings\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from lineartree import LinearTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "pd.set_option('display.max_columns', None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting to Google Drive and Current Working Directory"
      ],
      "metadata": {
        "id": "Dl6W0OG3wItX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# connecting to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LKtMO5nXteH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# connecting to current working directory\n",
        "os.chdir('/content/drive/MyDrive')\n",
        "print(f\"current working directory: {os.getcwd()}\")"
      ],
      "metadata": {
        "id": "0rx-NbnAiF-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Dataset (Train, Test, and Submission File)"
      ],
      "metadata": {
        "id": "9JIVaS0WwudZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading datasets\n",
        "train = pd.read_csv('train_erosion.csv')"
      ],
      "metadata": {
        "id": "9GBTrTJyiHBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(2)"
      ],
      "metadata": {
        "id": "j3qAaZuoA1vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "nVddpZdOyC7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Class Distribution of Target Variable"
      ],
      "metadata": {
        "id": "e_-SdHp085zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "map_class = {0:'No Gully/badland', 1:'Gully', 2:'Badland', 3:'Landslides'}\n",
        "train['classes'] = train['Code'].map(map_class)\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='classes', data=train, palette=\"husl\")\n",
        "plt.xlabel(\"Categories\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Frequency Distribution of Column\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZMGnDVNc4Mmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# total size of the train dataset\n",
        "train.shape"
      ],
      "metadata": {
        "id": "r44bFa2yxAxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Columns we won't be using\n",
        "not_needed_columns = ['UUID', 'sample_id', 'Unnamed: 0']\n",
        "bands_list = ['blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'thermal']\n"
      ],
      "metadata": {
        "id": "j_oXzXVWzfl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "lwNkzr_0w8vW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Aggregation (mean) of all similar columns.\n",
        "\n",
        "* So here I am taking all Blue, green, red, nir, swir1 and swir2 columns and multiplying them by their percentile and calculating their mean, then removing the original three columns themselves."
      ],
      "metadata": {
        "id": "r5VC6fsYeYtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_used_to_calculate_mean = []\n",
        "\n",
        "for count, band in enumerate(bands_list):\n",
        "    all_band_columns = [col for col in train.columns if bands_list[count] in col.lower()]\n",
        "\n",
        "    # get the probabiltity distribution value for each of the bands\n",
        "    value_1 = int(all_band_columns[0].split('_')[2][1:])/100\n",
        "    value_2 = int(all_band_columns[1].split('_')[2][1:])/100\n",
        "    value_3 = int(all_band_columns[2].split('_')[2][1:])/100\n",
        "\n",
        "    # calculate mean\n",
        "    train['mean_' + bands_list[count]] = ((train[all_band_columns[0]])*value_1 + (train[all_band_columns[1]])*value_2 + (train[all_band_columns[2]])*value_3)/3\n",
        "    train['mean_' + bands_list[count]] = train['mean_' + bands_list[count]].round(2)\n",
        "\n",
        "\n",
        "    # append the names of the columns used to calculate the mean\n",
        "    for column in all_band_columns:\n",
        "        columns_used_to_calculate_mean.append(column)"
      ],
      "metadata": {
        "id": "YH38VVpOe0n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying Aggregation to similar columns"
      ],
      "metadata": {
        "id": "wxIvsCctUtoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix_names = ['landform.moderate.mountain.smooth','wetlands.regularly-flooded','landform.upper.large.slope','sample','ndvi','nightlights.average',\n",
        " 'landform.terrace.smooth.plateau','bs','landform.middle.large.slope','nos','landform.alluvial.plain.pediplain',\n",
        " 'landform','landform.alluvial.or.coasttal.plain.pediplain','landform.steep.mountain.smooth','bioclim.var','slope','fapar','lcv.forest','cdr',\n",
        " 'landform.moderate.mountain.rough','landform.hills.rough.in.small.and.large.scale.','wv','downslope.curvature','evi',\n",
        " 'wetlands.permanent','clm','log1p.upstream.area','vbf','upslope.curvature','landform.alluvial.or.coasttal.plain.gentlest.lake.plain.playa',\n",
        " 'lcv','landform.dissected.terrace.moderate.plateau','pos.openess','landform.slope.in.and.around.terrace.or.plateau','dtm','snow.prob','ndsi',\n",
        " 'wetlands.cw','landform.hills.rough.in.small.and.large.scale','wetlands.groundwater-driven','landform.alluvial.fan..pediment..bajada..pediplain',\n",
        " 'landform.steep.mountain.rough']"
      ],
      "metadata": {
        "id": "Jozc35k1gMEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "agg_full_train = pd.DataFrame()\n",
        "\n",
        "for col in tqdm(prefix_names):\n",
        "    cols = [c for c in train.columns if col in c]\n",
        "\n",
        "    train_df_agg = train[cols].agg(['mean', 'max', 'min', 'std', 'nunique', 'skew', 'kurt'], axis=1)\n",
        "\n",
        "\n",
        "    train_df_agg.columns = [f'{col}_{c}' for c in train_df_agg.columns]\n",
        "\n",
        "    agg_full_train = pd.concat([agg_full_train, train_df_agg], axis=1)\n",
        "\n",
        "\n",
        "# append the aggregated data to the full_train dataset\n",
        "train = pd.concat([train, agg_full_train], axis=1)\n",
        "\n",
        "train.dropna(axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "QotyWUvNEaTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trying to see if aggregation (mean) of all similar columns would work.\n",
        "\n",
        "* So here I am taking all Blue, green, red, nir, swir1 and swir2 columns and dividing by the number of columns and then I would remove the original three columns themselves."
      ],
      "metadata": {
        "id": "kKqeDuQHxmAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_used_to_calculate_mean = []\n",
        "\n",
        "for count, band in enumerate(bands_list):\n",
        "    all_band_columns = [col for col in train.columns if bands_list[count] in col.lower()]\n",
        "\n",
        "    # get the probabiltity distribution value for each of the bands\n",
        "    value_1 = int(all_band_columns[0].split('_')[2][1:])/100\n",
        "    value_2 = int(all_band_columns[1].split('_')[2][1:])/100\n",
        "    value_3 = int(all_band_columns[2].split('_')[2][1:])/100\n",
        "\n",
        "    # calculate mean\n",
        "    train['mean_' + bands_list[count]] = ((train[all_band_columns[0]])*value_1 + (train[all_band_columns[1]])*value_2 + (train[all_band_columns[2]])*value_3)/3\n",
        "    train['mean_' + bands_list[count]] = train['mean_' + bands_list[count]].round(2)\n",
        "\n",
        "    # append the names of the columns used to calculate the mean\n",
        "    for column in all_band_columns:\n",
        "        columns_used_to_calculate_mean.append(column)"
      ],
      "metadata": {
        "id": "CD8_BOL8xmAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topographic Factors"
      ],
      "metadata": {
        "id": "xWbxV0G3P0nd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Topographic Wetness Index (TWI)"
      ],
      "metadata": {
        "id": "_k6PyxEy9V4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function for calculating TWI"
      ],
      "metadata": {
        "id": "n5mJa9maQri1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Calculating slope\n",
        "def calculate_slope(dem, cell_size):\n",
        "    dz_dx = np.gradient(dem) / cell_size\n",
        "    dz_dy = np.gradient(dem) / cell_size\n",
        "\n",
        "    slope_rad = np.arctan(np.sqrt(dz_dx**2 + dz_dy**2))\n",
        "    slope_deg = np.degrees(slope_rad)\n",
        "    return slope_deg\n",
        "\n",
        "\n",
        "def calculate_twi_from_csv(dem_df, cell_size):\n",
        "    # Read DEM data from CSV\n",
        "    dem = dem_df.values\n",
        "\n",
        "    # Calculate slope\n",
        "    slope_deg = calculate_slope(dem, cell_size)\n",
        "\n",
        "    # Calculate TWI\n",
        "    a = np.ones_like(dem) * cell_size**2\n",
        "    twi = np.log(a / np.tan(np.radians(slope_deg)))\n",
        "    return twi"
      ],
      "metadata": {
        "id": "heBbGUuF9eb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Appyling TWI function to dataset"
      ],
      "metadata": {
        "id": "pag7sBbhQxKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieving the elevation column\n",
        "dtm_col = [col for col in train.columns if 'dtm_elev' in col.lower()]\n",
        "\n",
        "# calculating TWI for train dataset\n",
        "twi =  calculate_twi_from_csv(dem_df= train[dtm_col[0]], cell_size=30)\n",
        "train['twi'] = twi"
      ],
      "metadata": {
        "id": "5t1j7tb-Qn8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combined slope length and slope angle (LS-factor)"
      ],
      "metadata": {
        "id": "uqnxjMTAVqMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LS-Factor Function"
      ],
      "metadata": {
        "id": "jUs9Be0TR4zO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_slope_length_factor(slope_length, slope_steepness, m=0.4):\n",
        "    ls_factor = (slope_length / 22.1)**m * (np.sin(np.radians(slope_steepness)) / 0.09)**1.3\n",
        "    return ls_factor\n",
        "\n",
        "def calculate_ls_factor_from_csv(df, cell_size):\n",
        "    # Read DEM data from CSV\n",
        "    slope_deg = df['slope_merit.dem_m_250m_s0..0cm_2017_2017_go_epsg.4326_v20231002'].values\n",
        "\n",
        "    # Calculate slope length (assuming constant cell size)\n",
        "    slope_length = np.sqrt(cell_size**2 + cell_size**2)\n",
        "\n",
        "    # Calculate LS factor\n",
        "    ls_factor = calculate_slope_length_factor(slope_length, slope_deg)\n",
        "    return ls_factor\n"
      ],
      "metadata": {
        "id": "LuBk5P8YV00W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Applying LS-Factor Function to dataset"
      ],
      "metadata": {
        "id": "FT4aYVwGRY8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ls = calculate_ls_factor_from_csv(train, 250)\n",
        "train['ls_factor'] = train_ls"
      ],
      "metadata": {
        "id": "GvGt6SJXR99B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SPI (Stream Power Index)"
      ],
      "metadata": {
        "id": "J3jn3pr_VxPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SPI Function"
      ],
      "metadata": {
        "id": "uEjknlYzSoDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_spi_from_csv(df):\n",
        "    # Calculate slope\n",
        "    slope_deg = df['slope_merit.dem_m_250m_s0..0cm_2017_2017_go_epsg.4326_v20231002'].values\n",
        "\n",
        "    # Calculate SPI\n",
        "    spi = np.sin(np.radians(slope_deg))\n",
        "    return spi"
      ],
      "metadata": {
        "id": "dyNVckY-VydV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Applying SPI function to dataset"
      ],
      "metadata": {
        "id": "-nGcBrVDSuNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_spi = calculate_spi_from_csv(train)\n",
        "train['spi'] = train_spi"
      ],
      "metadata": {
        "id": "v3DD6nofStVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vegetation Indices"
      ],
      "metadata": {
        "id": "THOJWGgELzkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SAVI (Soil Adjusted Vegetation Index)"
      ],
      "metadata": {
        "id": "9hjKEhuMTW2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVI = ((NIR - Red) / (NIR + Red + L)) x (1 + L)\n",
        "\n",
        "train['mean_savi'] = ((train['mean_nir'] - train['mean_red']) / (train['mean_nir'] + train['mean_red'] + 0.5) * (1 + 0.5))"
      ],
      "metadata": {
        "id": "fiMYludaTcvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WDVI (Weighted Difference Vegetation Index )\n",
        "* WDVI = NIR - (slope x RED)"
      ],
      "metadata": {
        "id": "c7iACj890rZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve the slope col\n",
        "slope_col = [col for col in train.columns if 'slope_merit' in col.lower()]\n",
        "\n",
        "# calculate WDVI\n",
        "train['WDVI'] = train['mean_nir'] - (train[slope_col[0]] * train['mean_red'])"
      ],
      "metadata": {
        "id": "nUgNXY7AcyfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NDWI (Normalized Difference Water Index)"
      ],
      "metadata": {
        "id": "AbAFt5KuWsIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 𝑁𝐼𝑅−𝑆𝑊𝐼𝑅1𝑁𝐼𝑅+𝑆𝑊𝐼𝑅1\n",
        "\n",
        "train['mean_ndwi'] = ((train['mean_nir'] - train['mean_swir1']) / (train['mean_nir'] + train['mean_swir1']))"
      ],
      "metadata": {
        "id": "rLLnJ8FYWrLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NDII (Normalized Difference Infrared Index )"
      ],
      "metadata": {
        "id": "9DIZuy-7XJ3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 𝑆𝑊𝐼𝑅1−𝑆𝑊𝐼𝑅2𝑆𝑊𝐼𝑅1+𝑆𝑊𝐼𝑅2\n",
        "\n",
        "train['mean_ndii'] = ((train['mean_swir1'] - train['mean_swir2']) / (train['mean_swir1'] + train['mean_swir2']))"
      ],
      "metadata": {
        "id": "R9SWTMWWXObT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SIWSI (Shortwave Infrared Water Stress Index)"
      ],
      "metadata": {
        "id": "_ruP9LYyXuYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 𝑁𝐼𝑅−𝑆𝑊𝐼𝑅2𝑁𝐼𝑅+𝑆𝑊𝐼𝑅2\n",
        "\n",
        "train['mean_siwsi'] = ((train['mean_nir'] - train['mean_swir2']) / (train['mean_nir'] + train['mean_swir2']))"
      ],
      "metadata": {
        "id": "GdLoRh5UXyza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tasseled Cap Brightness"
      ],
      "metadata": {
        "id": "5uuv1h-6bYj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['tasseled_cap_brightness'] =  (0.3037 * train['mean_blue'] + 0.2793 * train['mean_green'] + 0.4743 * \\\n",
        "                                     train['mean_red'] + 0.5585 * train['mean_nir'] + 0.5082 * \\\n",
        "                                     train['mean_swir1'] + 0.1863 * train['mean_swir2'])\n",
        "\n"
      ],
      "metadata": {
        "id": "SELaghpxbaGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tasseled Cap Greenness"
      ],
      "metadata": {
        "id": "yKJUc8ILb3GZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['tasseled_cap_greenness'] =  (-0.2848 * train['mean_blue'] - 0.2435 * train['mean_green'] - 0.5436 * train['mean_red'] + \\\n",
        "                                    0.7243 * train['mean_nir'] + 0.0840 * train['mean_swir1'] - 0.1800 * train['mean_swir2'])\n"
      ],
      "metadata": {
        "id": "Ct0l61TSb4ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tasseled Cap Wetness"
      ],
      "metadata": {
        "id": "O3Q1KsM7cjGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['tasseled_cap_wetness'] = (0.1509 * train['mean_blue'] + 0.1973 * train['mean_green']  + 0.3279 * train['mean_red'] + \\\n",
        "                                0.3406 * train['mean_nir'] - 0.7112 * train['mean_swir1'] - 0.4572 * train['mean_swir2'])\n"
      ],
      "metadata": {
        "id": "2XLFWPa1cjan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Climatic Factor"
      ],
      "metadata": {
        "id": "NYWH2yJBUbbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Land Surface Temperature (LST)"
      ],
      "metadata": {
        "id": "500jtOi_c022"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Land Surface Temperature Function"
      ],
      "metadata": {
        "id": "7PR2t9M0Am_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_land_surface_temperature(train, K1=774.8853, K2=1321.0789, wavelength=11.5, rho=0.98):\n",
        "    # Assuming 'mean_thermal' column is present in the 'train' DataFrame\n",
        "\n",
        "    radiance = (train['mean_thermal'].values * 0.0003342) + 0.1\n",
        "\n",
        "    # TOA Brightness Temperature\n",
        "    TOA_brightness_temp = K2 / np.log((K1 / radiance) + 1)\n",
        "\n",
        "    # Land Surface Temperature\n",
        "    LST = TOA_brightness_temp / (1 + (TOA_brightness_temp * wavelength) / (rho * 1.4388e4) * np.log((0.98 * 1.4388e4 * wavelength**5) / radiance + 1))\n",
        "\n",
        "    train['land_surface_temp'] = LST"
      ],
      "metadata": {
        "id": "KhwwBOk4AmaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Applying LST Function"
      ],
      "metadata": {
        "id": "BUvZ6r8VDCzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate LST on train dataset\n",
        "calculate_land_surface_temperature(train)"
      ],
      "metadata": {
        "id": "q6JGaXHqDGi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bands Combination"
      ],
      "metadata": {
        "id": "koOHhLWTHHYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load example data (you can replace this with your own dataset)\n",
        "features = ['mean_blue', 'mean_green', 'mean_red',\n",
        "            'mean_nir', 'mean_swir1', 'mean_swir2', 'mean_thermal', 'Code']\n",
        "\n",
        "\n",
        "df = train[features]\n",
        "\n",
        "markers =  {0: \"s\", 1: \"X\", 2: \"P\", 3: \"o\" }\n",
        "\n",
        "# Create scatterplot matrix\n",
        "sns.set(style=\"dark\")\n",
        "sns.pairplot(df, hue=\"Code\", markers=markers)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C-Z1aR5USust"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Indices using Interesting Bands"
      ],
      "metadata": {
        "id": "ocFZggcSWFRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interesting_bands = []\n",
        "\n",
        "### Making Band combinations of the bands we find interesting\n",
        "bands = ['mean_nir', 'mean_swir1', 'mean_swir2', 'mean_thermal']\n",
        "\n",
        "# Get all possible two combinations\n",
        "two_combinations = list(combinations(bands, 2))\n",
        "\n",
        "# Print the result\n",
        "for combo in two_combinations:\n",
        "    interesting_bands.append(combo)\n",
        "interesting_bands"
      ],
      "metadata": {
        "id": "QBaBjcP4-xtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for combo in interesting_bands:\n",
        "    train[combo[0]+'_'+combo[1]+ '_ratio'] = train[combo[0]] - (train[slope_col[0]] * train[combo[1]])\n",
        "    train[combo[1]+'_'+combo[0]+ '_ratio'] = train[combo[1]] - (train[slope_col[0]] * train[combo[0]])\n"
      ],
      "metadata": {
        "id": "RFKRxB3DH2fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Training Models (RF, XGBoost, LMT, CatBoost, BRT and LightGBM) on Dataset\n"
      ],
      "metadata": {
        "id": "Dv3y6zbwYmCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_columns = [col for col in train.columns\n",
        "                    if col not in not_needed_columns and col not in bands_list]\n",
        "training_columns.remove('Code')\n",
        "\n",
        "print(len(training_columns))"
      ],
      "metadata": {
        "id": "4o_j5qJEZt0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = train[training_columns]\n",
        "\n",
        "\n",
        "# Replace spaces with underscores\n",
        "training_dataset.columns = training_dataset.columns.str.replace(' ', '_')\n",
        "\n",
        "# Remove other special characters\n",
        "training_dataset.columns = training_dataset.columns.str.replace('[^a-zA-Z0-9]', '', regex=True)\n",
        "\n",
        "\n",
        "training_dataset = training_dataset.loc[:, ~training_dataset.columns.duplicated()]\n"
      ],
      "metadata": {
        "id": "g3JEHjd8Zc6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stratified_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# X, y  = training_dataset, train['Code']\n",
        "\n",
        "X, y  =  training_dataset, train['Code']\n",
        "\n",
        "\n",
        "\n",
        "feats = pd.DataFrame({'features': X.columns})\n",
        "gbm_predictions = []\n",
        "\n",
        "\n",
        "# F1 cv scores\n",
        "xgb_cv_score_ = 0\n",
        "lgb_cv_score_ = 0\n",
        "cat_cv_score_ = 0\n",
        "rf_cv_score_ = 0\n",
        "lmt_cv_score_ = 0\n",
        "brt_cv_score_ = 0\n",
        "\n",
        "# precision cv score\n",
        "xgb_precision_score_ = 0\n",
        "lgb_precision_score_ = 0\n",
        "cat_precision_score_ = 0\n",
        "rf_precision_score_ = 0\n",
        "lmt_precision_score_ = 0\n",
        "brt_precision_score_ = 0\n",
        "\n",
        "# recall cv score\n",
        "xgb_recall_score_ = 0\n",
        "lgb_recall_score_ = 0\n",
        "cat_recall_score_ = 0\n",
        "rf_recall_score_ = 0\n",
        "lmt_recall_score_ = 0\n",
        "brt_recall_score_ = 0\n",
        "\n",
        "\n",
        "for i,(tr_index,test_index) in enumerate(stratified_kf.split(X,y)):\n",
        "    print()\n",
        "    print(f'######### FOLD {i+1} / {stratified_kf.n_splits} ')\n",
        "\n",
        "    X_train,y_train = X.iloc[tr_index,:],y[tr_index]\n",
        "    X_test,y_test = X.iloc[test_index,:],y[test_index]\n",
        "\n",
        "\n",
        "    # Random forest model\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "    rf.fit(X_train,y_train)\n",
        "    rf_cv_score_ += f1_score(y_test, rf.predict(X_test), average='weighted') / stratified_kf.n_splits\n",
        "    rf_precision_score_ += precision_score(y_test, rf.predict(X_test), average='weighted') / stratified_kf.n_splits\n",
        "    rf_recall_score_ += recall_score(y_test, rf.predict(X_test), average='weighted') / stratified_kf.n_splits\n",
        "\n",
        "\n",
        "\n",
        "    # for xgb model\n",
        "    xgb_gbm = xgb.XGBClassifier(missing=-1,device='cuda')\n",
        "    xgb_gbm.fit(X_train,y_train, eval_set = [(X_test, y_test)], verbose=False)\n",
        "    xgb_cv_score_ += f1_score(y_test, xgb_gbm.predict(X_test), average='weighted') / stratified_kf.n_splits\n",
        "    xgb_precision_score_ += precision_score(y_test, xgb_gbm.predict(X_test), average='weighted') / stratified_kf.n_splits\n",
        "    xgb_recall_score_ += recall_score(y_test, xgb_gbm.predict(X_test), average='weighted') / stratified_kf.n_splits\n",
        "\n",
        "\n",
        "    #catboost model\n",
        "    cat_gbm = CatBoostClassifier(random_seed=24,silent=True)\n",
        "    cat_gbm.fit(X_train,y_train,eval_set = [(X_test, y_test)], verbose=False)\n",
        "    cat_cv_score_ += f1_score(y_test, cat_gbm.predict(X_test), average='weighted') / stratified_kf.n_splits\n",
        "    cat_precision_score_ += precision_score(y_test, cat_gbm.predict(X_test), average='weighted') / stratified_kf.n_splits\n",
        "    cat_recall_score_ += recall_score(y_test, cat_gbm.predict(X_test), average='weighted') / stratified_kf.n_splits\n",
        "\n",
        "\n",
        "    # Boosteed regression tree model\n",
        "    brt_model = GradientBoostingClassifier(random_state=42)\n",
        "    brt_model.fit(X_train,y_train)\n",
        "    brt_cv_score_ += f1_score(y_test, brt_model.predict(X_test), average='weighted') / stratified_kf.n_splits\n",
        "    brt_precision_score_ += precision_score(y_test, brt_model.predict(X_test), average='weighted') / stratified_kf.n_splits\n",
        "    brt_recall_score_ += recall_score(y_test, brt_model.predict(X_test), average='weighted') / stratified_kf.n_splits\n",
        "\n",
        "\n",
        "    # for lightgbm model\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"[LightGBM] \\[Warning\\] No further splits with positive gain, best gain: -inf\")\n",
        "    params  = {'random_state':42,'verbose': -1}\n",
        "    lgb_gbm = lgb.LGBMClassifier(**params)\n",
        "    lgb_gbm.fit(X_train,y_train,eval_set = [(X_test, y_test)])\n",
        "    lgb_cv_score_ += f1_score(y_test, lgb_gbm.predict(X_test), average='weighted') / stratified_kf.n_splits\n",
        "    lgb_precision_score_ += precision_score(y_test, lgb_gbm.predict(X_test), average='weighted') / stratified_kf.n_splits\n",
        "    lgb_recall_score_ += recall_score(y_test, lgb_gbm.predict(X_test), average='weighted') / stratified_kf.n_splits\n",
        "\n",
        "\n",
        "print( ' RF CV F1 Score : ', rf_cv_score_)\n",
        "print( ' XGB CV F1 Score : ', xgb_cv_score_)\n",
        "print( ' LGB CV F1 Score : ', lgb_cv_score_)\n",
        "print( ' CAT CV F1 Score  : ', cat_cv_score_)\n",
        "print( ' BRT CV F1 Score  : ', brt_cv_score_)"
      ],
      "metadata": {
        "id": "plDLRqlrZIwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_scores = [rf_cv_score_, xgb_cv_score_, lgb_cv_score_, brt_cv_score_, cat_cv_score_]\n",
        "precision_scores = [rf_precision_score_, xgb_precision_score_, lgb_precision_score_, brt_precision_score_, cat_precision_score_]\n",
        "recall_scores = [rf_recall_score_, xgb_recall_score_, lgb_recall_score_, brt_recall_score_, cat_recall_score_]\n",
        "\n",
        "model_names = ['Random Forest', 'XGBoost', 'LightGBM', 'BRT', 'CatBoost']\n",
        "\n",
        "evaluation_df = pd.DataFrame({'Model': model_names,  'Precision Score': precision_scores, 'Recall Score': recall_scores, 'F1 Score': f1_scores})\n",
        "\n",
        "evaluation_df.sort_values(by='F1 Score', ascending=False)"
      ],
      "metadata": {
        "id": "CNCzhp_9uy8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Columns used for training as determined by  Recursive Feature Elimination Algorithm"
      ],
      "metadata": {
        "id": "QqbQXB1g0n8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_columns = ['Year',\n",
        " 'WDVI',\n",
        " 'nightlights.average_min',\n",
        " 'mean_nir_mean_swir2_ratio',\n",
        " 'nightlights.average_max',\n",
        " 'Coor_y',\n",
        " 'clm_std',\n",
        " 'mean_nir_mean_thermal_ratio',\n",
        " 'mean_swir1_mean_thermal_ratio',\n",
        " 'nightlights.average_viirs.v21_m_500m_s_{year}0101_{year}1231_go_epsg4326_v20230318',\n",
        " 'nightlights.average_mean',\n",
        " 'mean_thermal_mean_swir2_ratio',\n",
        " 'fapar_max',\n",
        " 'bs_mean',\n",
        " 'mean_swir1_mean_swir2_ratio',\n",
        " 'ndsi_min',\n",
        " 'bioclim.var_chelsa.bio2_m_1km_s_1981_2010_go_epsg.4326_v20231002',\n",
        " 'clm_max',\n",
        " 'evi_max',\n",
        " 'dtm_max',\n",
        " 'bioclim.var_min',\n",
        " 'clm_lst_mod11a2.daytime_p95_1km_s0..0cm_{year}_v1.2',\n",
        " 'clm_min',\n",
        " 'bs_max',\n",
        " 'bs_min',\n",
        " 'dtm_skew',\n",
        " 'bs_glad.landsat.seasconv.m.yearly_sum_30m_s_{year}0101_{year}1231_eu_epsg.3035_v20231127',\n",
        " 'mean_swir2_mean_thermal_ratio',\n",
        " 'ndvi_glad.landsat.seasconv.m.yearly_p75_30m_s_{year}0101_{year}1231_eu_epsg.3035_v20231127',\n",
        " 'pos.openess_min',\n",
        " 'evi_glad.landsat.seasconv_m_30m_s_{year}0501_{year}0630_eu_epsg.3035_v20231127',\n",
        " 'wv_mcd19a2v061.seasconv.m.yearly_p25_1km_s_{year}0101_{year}1231_go_epsg.4326_v20230619',\n",
        " 'mean_thermal_mean_swir1_ratio',\n",
        " 'Coor_x',\n",
        " 'pos.openess_max',\n",
        " 'dtm_elev.lowestmode_gedi.eml_mf_30m_0..0cm_2000..2018_eumap_epsg3035_v0.3',\n",
        " 'pos.openess_merit.dem_m_250m_s0..0cm_2017_2017_go_epsg.4326_v20231002',\n",
        " 'mean_nir_mean_swir1_ratio',\n",
        " 'ndsi_glad.landsat.seasconv_m_30m_s_{year}0701_{year}0831_eu_epsg.3035_v20231127',\n",
        " 'pos.openess_mean',\n",
        " 'dtm_kurt',\n",
        " 'mean_swir2_mean_swir1_ratio',\n",
        " 'wv_min',\n",
        " 'dtm_mean',\n",
        " 'bioclim.var_chelsa.bio3_m_1km_s_1981_2010_go_epsg.4326_v20231002',\n",
        " 'tasseled_cap_brightness',\n",
        " 'evi_std',\n",
        " 'wv_mean',\n",
        " 'clm_lst_mod11a2.nighttime_sd_1km_s0..0cm_{year}_v1.2',\n",
        " 'landform_mean',\n",
        " 'ndvi_max',\n",
        " 'clm_lst_mod11a2.nighttime_p95_1km_s0..0cm_{year}_v1.2',\n",
        " 'ndsi_glad.landsat.seasconv_m_30m_s_{year}0501_{year}0630_eu_epsg.3035_v20231127',\n",
        " 'clm_lst_mod11a2.nighttime_p05_1km_s0..0cm_{year}_v1.2',\n",
        " 'wv_max',\n",
        " 'landform.dissected.terrace.moderate.plateau_mean',\n",
        " 'wv_skew',\n",
        " 'wv_mcd19a2v061.seasconv.m.yearly_sd_1km_s_{year}0101_{year}1231_go_epsg.4326_v20230619',\n",
        " 'slope_nunique',\n",
        " 'clm_skew',\n",
        " 'vbf_merit.dem_m_250m_s0..0cm_2017_2017_go_epsg.4326_v20231002',\n",
        " 'dtm_std',\n",
        " 'landform_terrain.class_c_250m_s0..0cm_2017_2018_go_epsg.4326_v20231002',\n",
        " 'ndsi_glad.landsat.seasconv_m_30m_s_{year}0901_{year}1031_eu_epsg.3035_v20231127',\n",
        " 'slope_std',\n",
        " 'fapar_glad.landsat.seasconv_m_30m_s_{year}0301_{year}0430_eu_epsg.3035_v20231127',\n",
        " 'snow.prob_mean',\n",
        " 'mean_swir1',\n",
        " 'fapar_std',\n",
        " 'twi',\n",
        " 'clm_lst_mod11a2.daytime_p05_1km_s0..0cm_{year}_v1.2',\n",
        " 'evi_glad.landsat.seasconv_m_30m_s_{year}0701_{year}0831_eu_epsg.3035_v20231127',\n",
        " 'bioclim.var_kurt',\n",
        " 'bioclim.var_max',\n",
        " 'vbf_mean',\n",
        " 'lcv_mean',\n",
        " 'landform_skew',\n",
        " 'ndvi_glad.landsat.seasconv.m.yearly_p50_30m_s_{year}0101_{year}1231_eu_epsg.3035_v20231127',\n",
        " 'fapar_glad.landsat.seasconv_m_30m_s_{year}0501_{year}0630_eu_epsg.3035_v20231127',\n",
        " 'fapar_glad.landsat.seasconv_m_30m_s_{year}0901_{year}1031_eu_epsg.3035_v20231127',\n",
        " 'mean_savi',\n",
        " 'clm_lst_mod11a2.nighttime_p50_1km_s0..0cm_{year}_v1.2',\n",
        " 'lcv_globalcropland_bowen.et.al_p_1km_s0..0cm_{year}_v0.1',\n",
        " 'wetlands.regularly-flooded_upmc.wtd_p_250m_b0..200cm_2010_2015_go_epsg.4326_v20231002',\n",
        " 'mean_thermal',\n",
        " 'snow.prob_esacci.m12_p90_500m_s_2000_2012_go_epsg.4326_v20231002',\n",
        " 'snow.prob_std',\n",
        " 'evi_glad.landsat.seasconv_m_30m_s_{year}0301_{year}0430_eu_epsg.3035_v20231127',\n",
        " 'wv_mcd19a2v061.seasconv.m.yearly_p75_1km_s_{year}0101_{year}1231_go_epsg.4326_v20230619',\n",
        " 'landform.middle.large.slope_terrain.class_p_250m_s0..0cm_2017_2018_go_epsg.4326_v20231002',\n",
        " 'mean_swir2_mean_nir_ratio',\n",
        " 'vbf_max',\n",
        " 'ndvi_nunique',\n",
        " 'mean_nir',\n",
        " 'clm_accum.precipitation_chelsa.annual_m_1km_s0..0cm_{year}_v2.1',\n",
        " 'clm_lst_mod11a2.daytime_p50_1km_s0..0cm_{year}_v1.2',\n",
        " 'snow.prob_max',\n",
        " 'snow.prob_esacci.m10_p90_500m_s_2000_2012_go_epsg.4326_v20231002',\n",
        " 'slope_merit.dem_m_250m_s0..0cm_2017_2017_go_epsg.4326_v20231002',\n",
        " 'bioclim.var_skew',\n",
        " 'fapar_glad.landsat.seasconv_m_30m_s_{year}0701_{year}0831_eu_epsg.3035_v20231127',\n",
        " 'snow.prob_esacci.m01_p90_500m_s_2000_2012_go_epsg.4326_v20231002',\n",
        " 'snow.prob_esacci.m10_sd_500m_s_2000_2012_go_epsg.4326_v20231002',\n",
        " 'bioclim.var_chelsa.bio12_m_1km_s_1981_2010_go_epsg.4326_v20231002',\n",
        " 'mean_blue',\n",
        " 'snow.prob_min',\n",
        " 'wv_std',\n",
        " 'fapar_mean',\n",
        " 'vbf_min',\n",
        " 'snow.prob_esacci.m09_sd_500m_s_2000_2012_go_epsg.4326_v20231002',\n",
        " 'slope_min',\n",
        " 'lcv_std',\n",
        " 'snow.prob_esacci.m12_sd_500m_s_2000_2012_go_epsg.4326_v20231002',\n",
        " 'lcv_max',\n",
        " 'dtm_lithology_usgs.ecotapestry.intermediate.plutonics_p_250m_s0..0cm_2014_v1.0',\n",
        " 'clm_nunique',\n",
        " 'clm_lst_mod11a2.daytime_sd_1km_s0..0cm_{year}_v1.2',\n",
        " 'dtm_lithology_usgs.ecotapestry.siliciclastic.sedimentary_p_250m_s0..0cm_2014_v1.0',\n",
        " 'ndsi_glad.landsat.seasconv_m_30m_s_{year}0101_{year}0228_eu_epsg.3035_v20231127',\n",
        " 'upslope.curvature_min',\n",
        " 'ndsi_glad.landsat.seasconv_m_30m_s_{year}0301_{year}0430_eu_epsg.3035_v20231127',\n",
        " 'wetlands.regularly-flooded_min',\n",
        " 'ndsi_mean',\n",
        " 'wv_kurt',\n",
        " 'downslope.curvature_mean',\n",
        " 'clm_mean',\n",
        " 'wetlands.cw_upmc.wtd_c_250m_b0..200cm_2010_2015_go_epsg.4326_v20231002',\n",
        " 'downslope.curvature_merit.dem_m_250m_s0..0cm_2017_2017_go_epsg.4326_v20231002',\n",
        " 'ndvi_glad.landsat.seasconv.m.yearly_p25_30m_s_{year}0101_{year}1231_eu_epsg.3035_v20231127',\n",
        " 'log1p.upstream.area_merit.hydro_m_250m_b0..0cm_2017_2017_go_epsg.4326_v20231002',\n",
        " 'snow.prob_esacci.m03_p90_500m_s_2000_2012_go_epsg.4326_v20231002',\n",
        " 'log1p.upstream.area_min',\n",
        " 'land_surface_temp',\n",
        " 'clm_kurt',\n",
        " 'snow.prob_esacci.m11_sd_500m_s_2000_2012_go_epsg.4326_v20231002',\n",
        " 'ndsi_std',\n",
        " 'ndvi_mean',\n",
        " 'upslope.curvature_max',\n",
        " 'snow.prob_esacci.m03_sd_500m_s_2000_2012_go_epsg.4326_v20231002',\n",
        " 'mean_red',\n",
        " 'wetlands.regularly-flooded_mean',\n",
        " 'slope_mean',\n",
        " 'ls_factor',\n",
        " 'snow.prob_esacci.m05_sd_500m_s_2000_2012_go_epsg.4326_v20231002',\n",
        " 'mean_swir1_mean_nir_ratio',\n",
        " 'fapar_glad.landsat.seasconv_m_30m_s_{year}0101_{year}0228_eu_epsg.3035_v20231127',\n",
        " 'wv_mcd19a2v061.seasconv.m.yearly_p50_1km_s_{year}0101_{year}1231_go_epsg.4326_v20230619',\n",
        " 'downslope.curvature_max',\n",
        " 'snow.prob_esacci.m01_sd_500m_s_2000_2012_go_epsg.4326_v20231002',\n",
        " 'wetlands.regularly-flooded_max',\n",
        " 'fapar_min',\n",
        " 'log1p.upstream.area_mean',\n",
        " 'dtm_lithology_usgs.ecotapestry.acid.volcanic_p_250m_s0..0cm_2014_v1.0',\n",
        " 'snow.prob_esacci.m11_p90_500m_s_2000_2012_go_epsg.4326_v20231002',\n",
        " 'snow.prob_esacci.m02_p90_500m_s_2000_2012_go_epsg.4326_v20231002']\n",
        "\n",
        "\n",
        "print(len(training_columns))"
      ],
      "metadata": {
        "id": "pu69L5URk0fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean the names of the columns"
      ],
      "metadata": {
        "id": "twIJW2zT4D8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = train[training_columns]\n",
        "\n",
        "# Replace spaces with underscores\n",
        "training_dataset.columns = training_dataset.columns.str.replace(' ', '_')\n",
        "\n",
        "# Remove other special characters\n",
        "training_dataset.columns = training_dataset.columns.str.replace('[^a-zA-Z0-9]', '', regex=True)\n",
        "\n",
        "\n",
        "training_dataset = training_dataset.loc[:, ~training_dataset.columns.duplicated()]\n"
      ],
      "metadata": {
        "id": "cgiqst3qbKWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelling"
      ],
      "metadata": {
        "id": "e8GRbZ4yWc0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Voting Classifier Cross Validation"
      ],
      "metadata": {
        "id": "v9mbIEYIQwCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stratified_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "X, y  = training_dataset, train['Code']\n",
        "\n",
        "feats = pd.DataFrame({'features': X.columns})\n",
        "gbm_predictions = []\n",
        "cv_score_ = 0\n",
        "oof_preds = np.zeros((train.shape[0],))\n",
        "\n",
        "\n",
        "all_preds = []\n",
        "all_true_labels = []\n",
        "\n",
        "for i,(tr_index,test_index) in enumerate(stratified_kf.split(X,y)):\n",
        "    print()\n",
        "    print(f'######### FOLD {i+1} / {stratified_kf.n_splits} ')\n",
        "\n",
        "    X_train,y_train = X.iloc[tr_index,:],y[tr_index]\n",
        "    X_test,y_test = X.iloc[test_index,:],y[test_index]\n",
        "\n",
        "\n",
        "    # for xgb model\n",
        "    xgb_gbm = xgb.XGBClassifier(\n",
        "                            gamma = 0.23,\n",
        "                            n_estimators=2000,\n",
        "                            max_depth=12,\n",
        "                            learning_rate=0.01,\n",
        "                            subsample=0.8,\n",
        "                            colsample_bytree=0.4,\n",
        "                            missing=-1,\n",
        "                            device='cuda',\n",
        "                            )\n",
        "\n",
        "\n",
        "    # for lightgbm model\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"[LightGBM] \\[Warning\\] No further splits with positive gain, best gain: -inf\")\n",
        "    params  = {\n",
        "   'boosting_type': 'dart','n_estimators':2500,'objective': 'multiclass',\n",
        "    'learning_rate':0.02, 'num_leaves':15,'reg_alpha':0,'reg_lambda':7,\n",
        "    'max_depth':5,\n",
        "    'random_state':42,'verbose': -1}\n",
        "    lgb_gbm = lgb.LGBMClassifier(**params)\n",
        "\n",
        "\n",
        "\n",
        "    #catboost classifier\n",
        "    cat_gbm = CatBoostClassifier(\n",
        "        loss_function='MultiClass',\n",
        "        depth = 6,\n",
        "        task_type=\"GPU\",\n",
        "        learning_rate=0.01,\n",
        "        iterations=4000,\n",
        "        od_type=\"Iter\",\n",
        "        random_seed=24,\n",
        "        silent=True,\n",
        "    )\n",
        "\n",
        "\n",
        "    # voting\n",
        "    voting_classifier = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('xgboost', xgb_gbm),\n",
        "        ('lightgbm', lgb_gbm),\n",
        "        ('catboost', cat_gbm),\n",
        "    ],\n",
        "    voting='soft',\n",
        "    )\n",
        "    voting_classifier.fit(X_train, y_train)\n",
        "\n",
        "    cv_score_ += f1_score(y_test, voting_classifier.predict(X_test), average='weighted') / stratified_kf.n_splits\n",
        "    fold_preds = voting_classifier.predict(X_test)\n",
        "    all_preds.extend(fold_preds)\n",
        "    all_true_labels.extend(y_test)\n",
        "\n",
        "\n",
        "print(' CV F1_Score : ', cv_score_)\n"
      ],
      "metadata": {
        "id": "bSb74Z0vOBjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_cm = confusion_matrix(all_true_labels, all_preds)\n",
        "labels = ['No Gully/badland', 'Gully', 'Badland', 'Landslides']\n",
        "plt.figure(figsize=(15, 5))\n",
        "sns.heatmap(overall_cm, annot=True, fmt=\"d\", cmap=\"crest\", xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n"
      ],
      "metadata": {
        "id": "wdmbujkI_CRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string_class_preds = ['No Gully/badland' if label == 0 else\n",
        "                'Gully' if label == 1 else 'Badland' if label == 2 else\n",
        "                'Landslides' for label in all_preds]\n",
        "\n",
        "string_class_true = ['No Gully/badland' if label == 0 else\n",
        "                'Gully' if label == 1 else 'Badland' if label == 2 else\n",
        "                'Landslides' for label in all_true_labels]"
      ],
      "metadata": {
        "id": "jWA8G6n2Cco4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(string_class_true, string_class_preds)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "bESNx8HmBbCI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}